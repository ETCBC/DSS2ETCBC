{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6wA-AnlJSf7"
   },
   "source": [
    "# DSS2ETCBC: the lexemes and parts of speech of the biblical scrolls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Martijn Naaijer and Jarod Jacobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNjAyp-bJSf9"
   },
   "source": [
    "The text-fabric package containing the Dead Sea Scrolls has a variety of word features, such as number, gender, person, and lexeme. One of the goals of the CACCHT project is to integrate the scrolls in the ETCBC database by encoding them in the ETCBC format. In a previous blogpost we explained how POS tagging of Hebrew texts can be done using an LSTM network. In this blogpost we show how the lexemes of the words in the biblical scrolls can be converted from the encoding done by Abegg to the ETCBC lexemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sgOHKoVJSf-"
   },
   "source": [
    "## General approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jHfsxF-JSf_"
   },
   "source": [
    "Various features in the dss package can be converted straightforwardly to ETCBC format. An example is number. We assume that verb forms in the first person in the dss package correspond one to one with the first person in the ETCBC database. \n",
    "\n",
    "There are features, for which this correspondence cannot be used straightforwardly. An example is the word feature part of speech. In the dss package the is the value \"ptcl\" (particle). This value corresponds with \"conj\" (conjunction), and \"prep\" (preposition), among other things.\n",
    "\n",
    "A similar thing is going on with the feature lexeme. In the ETCBC database, the lexemes are based on KBL, but in the course of time a whole range of improvements have been implemented, based on ongoing research. Therefore, it is understandable, that there is not always a one to one correspondence between the lexemes of the dss package and the BHSA. An example is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how can we assign the ETCBC values of the feature lexeme to the Scrolls? In the case of the biblical scrolls one can simply use lexemes used for the same words in corresponding verses. For instance, the scroll 4Q2 contains the text of Genesis. Its text of verse 1:1 is BR>CJT BR> >LHJM >T HCMJM W>T H>RY, which is identical to the text of Genesis 1:1 in the BHSA. In this case, we can simply give each word the lexeme of the same word in the BHSA. In many cases, however, the text of the scrolls deviates more or less with that of the BHSA.\n",
    "\n",
    "For this, we use sequence alignment. This means that two strings are arranged in such a way that similar parts are identified. Alignment of sequences is used often in biology, if one wants to identify similarities and differences in DNA strings or proteins. Sequence alignment techniques are often based on dynamic programming, such as the Smith Waterman Algorithm. In the package [biopython](https://towardsdatascience.com/pairwise-sequence-alignment-using-biopython-d1a9d0ba861f) a whole range of algorithms for the study of biological sequences are implemented. We use this package, and the module pairwise2, which is contained in it, to align biblical verses in the DSS and the BHSA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice , this looks as follows. As an example we look at Isaiah 48:2 in the BHSA and 1QIsaa. \n",
    "\n",
    "    KJ- M <JR H Q-DC NQR>W W <L >L-HJ JFR>L NSMKW JHWH YB>WT CMW   BHSA\n",
    "    \n",
    "    KJ> M <JR H QWDC NQR>W W <L >LWHJ JFR>L NSMKW JHWH YB>WT CMW   1QIsaa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the text of these verses is very similar, but there are also some differences. Similar parts in the verses are put together, resulting in two sequences of equal length. on the place of the extra matres lectiones in 1QIsaa, one finds a \"-\" in the BHSA.\n",
    "\n",
    "Now, we look at every character in 1QIsaa, and check the lexeme of the corresponding character in the BHSA. If more than half of the characters in a word in the scroll corresponds with one lexeme in the BHSA, we give the value of this lexeme to the word in the scroll. In the case of the first word in Isaiah 48:2, the K and J correspond to a character in the corresponding word with the lexeme KJ, and the character > does not correspond with a word in the BHSA. The result is that 2 out of 3 characters correspond with the lexeme KJ, which is more than 50%, so the first word KJ> in this verse of the scroll gets the value KJ. This approach works well in practice, but it does not result in a value for each word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example is Isaiah 42:23 in 1QIsaa: \n",
    "\n",
    "    MJ- BKM- --J>ZJN Z->T --JQCB W JCM< L >XWR    BHSA\n",
    "\n",
    "    MJ> BKMH W J>ZJN ZW>T W JQCB W JCM< L >XWR    1QIsaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the word \"W\" occurs three times in 1QIsaa, and only once in the BHSA. This means, that in the first two cases we cannot give these words an ETCBC lexeme. In this case, we can proceed by checking the lexeme \"W\" has in these cases in the dss module, which is \"W:\", and then find out with which lexeme \"W:\" corresponds in the BHSA most often. This is the BHSA lexeme \"W\", which can be given to the unmatched cases of \"W\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure works well in practice, but it is not infallible. In some cases the alignment is unfortunate, in other cases there is no one to one mapping between lexemes or the dss package has different definitions of what is a word. For instance, in the case of place names consisting of two words, the dss package treats them as distinct words, whereas the BHSA has one lexeme in general. To bring the resulting dataset to \"ETCBC standard\", more steps are needed. These can consist of additional automatic processing and/or manual cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach is used for the feature part of speech. You can find the resulting dataset in the [github repository](https://github.com/ETCBC/DSS2ETCBC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgX33l2HJSgB"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EinVOh6JSgC"
   },
   "source": [
    "The package biopython is used for aligning sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBCeUZfGJSgD"
   },
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dss package is loaded. The classes L, T and F, that are used here are renamed. In that case we can use them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "colab_type": "code",
    "id": "hqTV83XqJSgF",
    "outputId": "2c7023c2-01b2-47e4-bc22-948cf7a49c73"
   },
   "outputs": [],
   "source": [
    "from tf.app import use\n",
    "A = use('dss', hoist=globals())\n",
    "\n",
    "# give the relevant classes for the DSS new names\n",
    "Ldss = L\n",
    "Tdss = T\n",
    "Fdss = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "colab_type": "code",
    "id": "LvP1FezqJSgH",
    "outputId": "aa18a9f8-2f91-4ad8-885b-3aa469357175"
   },
   "outputs": [],
   "source": [
    "from tf.app import use\n",
    "A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary book_dict_bhsa_dss provides a mapping between the booknames of the BHSA and dss packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggryUfETJSgJ"
   },
   "outputs": [],
   "source": [
    "book_dict_bhsa_dss = {'Genesis':      'Gen',\n",
    "             'Exodus':       'Ex',\n",
    "             'Leviticus':    'Lev',\n",
    "             'Numbers':      'Num',\n",
    "             'Deuteronomy':  'Deut',\n",
    "             'Joshua':       'Josh',\n",
    "             'Judges':       'Judg',\n",
    "             '1_Samuel':     '1Sam',\n",
    "             '2_Samuel':     '2Sam',\n",
    "             '1_Kings':      '1Kgs',\n",
    "             '2_Kings':      '2Kgs',\n",
    "             'Isaiah':       'Is',\n",
    "             'Jeremiah':     'Jer',\n",
    "             'Ezekiel':      'Ezek',\n",
    "             'Hosea':        'Hos',\n",
    "             'Joel':         'Joel',\n",
    "             'Amos':         'Amos',\n",
    "             'Obadiah':      'Obad',\n",
    "             'Jonah':        'Jonah',\n",
    "             'Micah':        'Mic',\n",
    "             'Nahum':        'Nah',\n",
    "             'Habakkuk':     'Hab',\n",
    "             'Zephaniah':    'Zeph',\n",
    "             'Haggai':       'Hag',\n",
    "             'Zechariah':    'Zech',\n",
    "             'Malachi':      'Mal',\n",
    "             'Psalms':       'Ps',\n",
    "             'Job':          'Job',\n",
    "             'Proverbs':     'Prov',\n",
    "             'Ruth':         'Ruth',\n",
    "             'Song_of_songs':'Song',\n",
    "             'Ecclesiastes': 'Eccl',\n",
    "             'Lamentations': 'Lam',\n",
    "             'Daniel':       'Dan',\n",
    "             'Ezra':         'Ezra',\n",
    "             '2_Chronicles': '2Chr'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the reversed way can be useful as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "MxOF0rCxJSgK",
    "outputId": "11d7ac4e-69cf-4f49-8e36-dfdba40e438d"
   },
   "outputs": [],
   "source": [
    "book_dict_dss_bhsa = {v: k for k, v in book_dict_bhsa_dss.items()}\n",
    "print(book_dict_dss_bhsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RY7E0zKkJSgO"
   },
   "source": [
    "We define some helper functions. align_verses takes two sequences das input and aligns them. It returns the aligned sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhUcgXZDJSgO"
   },
   "outputs": [],
   "source": [
    "def align_verses(bhsa_data, dss_data):\n",
    "    \n",
    "    seq_bhsa = ' '.join(bhsa_data).strip()\n",
    "    seq_dss = ' '.join(dss_data).strip()\n",
    "        \n",
    "    seq1 = Seq(seq_bhsa) \n",
    "    seq2 = Seq(seq_dss)\n",
    "    \n",
    "    alignments = pairwise2.align.globalxx(seq1, seq2)\n",
    "    \n",
    "    bhsa_al = (alignments[0][0]).strip(' ')\n",
    "    dss_al = (alignments[0][1]).strip(' ')\n",
    "        \n",
    "    return bhsa_al, dss_al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function most_frequent takes a list with lexemes as input, and returns the most frequent lexeme, together with its frequency in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2J110ZCJSgQ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(lex_list): \n",
    "    occurrence_count = Counter(lex_list) \n",
    "    lex = occurrence_count.most_common(1)[0][0]\n",
    "    count = occurrence_count.most_common(1)[0][1]\n",
    "    return(lex, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function produce_value, the value of the POS or lexeme in the ETCBC format is retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_value(key, chars_to_feat_etcbc, chars_to_feat_dss):\n",
    "    # check for each consonant what the value of the feature is of the word of the corresponding consonant in the BHSA \n",
    "    if len(chars_to_feat_etcbc[(key[0], key[1], key[2])]) > 0:\n",
    "        all_feat_etcbc = chars_to_feat_etcbc[(key[0], key[1], key[2])]\n",
    "        \n",
    "        # check for a word to which lexeme in the bhsa it corresponds with most of its characters\n",
    "        feat_etcbc_proposed, count = most_frequent(all_feat_etcbc)\n",
    "        \n",
    "        # an etcbc-lexeme is assigned only to a word if more than half of its consonants\n",
    "        # corresponds with a word in the BHSA    \n",
    "        if len(list(word)) == 0:\n",
    "            feat_etcbc = ''\n",
    "\n",
    "        elif (count / len(list(word))) > 0.5:\n",
    "            feat_etcbc = feat_etcbc_proposed\n",
    "\n",
    "        else:\n",
    "            feat_etcbc = ''\n",
    "            \n",
    "    else:\n",
    "        feat_etcbc = ''\n",
    "        \n",
    "    if len(chars_to_feat_dss[key]) > 0:\n",
    "        \n",
    "        all_feat_dss = chars_to_feat_dss[key]\n",
    "        feat_dss, count = most_frequent(all_feat_dss)\n",
    "        \n",
    "    else:\n",
    "        feat_dss = ''\n",
    "        \n",
    "    return feat_dss, feat_etcbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7onUcpNtJSgR"
   },
   "source": [
    "## Prepare scrolls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Maa9xwGJSgS"
   },
   "source": [
    "In the next cell the text and lexemes of the Great Isaiah scroll are extracted from the DSS package, and some minor manipulations are done with the text.\n",
    "\n",
    "For every character in the text of the scroll 1QIsaa, we look up what the lexeme is of the word in which the character occurs. This information is saved in the dictionary lexemes_dss. The keys of this dict are the verse numbers of Isaiah. Each value is a list with the lexemes, one for each character in the verse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data retrieved in the following cell consists partly of reconstructions of scrolls. Other features in the package, not discussed here, deal with which part of the text can be read on the scrolls and which part is reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5Kd8wOsJSgS"
   },
   "outputs": [],
   "source": [
    "# In dss_data_dict, the text of each verse in the biblical scrolls is collected\n",
    "dss_data_dict = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "lexemes_dss = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "pos_dss = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "ids_dss = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "\n",
    "for scr in Fdss.otype.s('scroll'):\n",
    "    scroll_name = Tdss.scrollName(scr)\n",
    "    \n",
    "    words = Ldss.d(scr, 'word')\n",
    "        \n",
    "    for w in words:\n",
    "            \n",
    "        # exclude fragmentary data, these chapters start with 'f'\n",
    "        chapter = Ldss.u(w, 'chapter')\n",
    "        \n",
    "        bo = Fdss.book.v(w) \n",
    "        \n",
    "        if bo == None or bo not in book_dict_dss_bhsa:\n",
    "            continue\n",
    "        \n",
    "        if (Fdss.chapter.v(w))[0] == 'f':\n",
    "            continue\n",
    "        \n",
    "        # Do a bit of preprocessing\n",
    "        if Fdss.glyphe.v(w) != None:\n",
    "            \n",
    "            lexeme = Fdss.glexe.v(w)  \n",
    "            glyphs = Fdss.glyphe.v(w)\n",
    "            \n",
    "            # dummy value, check what happens here\n",
    "            if lexeme == None:\n",
    "                lexeme = 'XXX'\n",
    "                \n",
    "            # the consonant '#' is used for both 'C' and 'F'. We check in the lexeme\n",
    "            # to which of the two alternatives it should be converted. This appproach is crude, \n",
    "            # but works well in practice.\n",
    "            if '#' in glyphs:                    \n",
    "                if 'C' in lexeme:                        \n",
    "                    glyphs = glyphs.replace('#', 'C')                        \n",
    "                if 'F' in lexeme:\n",
    "                    glyphs = glyphs.replace('#', 'F')                        \n",
    "                # replace # by C in other cases\n",
    "                else:\n",
    "                    glyphs = glyphs.replace('#', 'C')\n",
    "            \n",
    "            # Some characters are removed or replaced\n",
    "            glyphs = glyphs.replace(u'\\xa0', u' ').replace(\"'\", \"\").replace(\"k\", \"K\").replace(\"n\", \"N\").replace(\"m\", \"M\").replace(\"y\", \"Y\").replace(\"p\", \"P\")   \n",
    "                \n",
    "            dss_book = Fdss.book.v(w)\n",
    "            bhsa_book_name = book_dict_dss_bhsa[dss_book]\n",
    "            \n",
    "            # replace(' ', '') is needed for strange case in Exodus 13:16 with a space in the word\n",
    "            dss_data_dict[(bhsa_book_name, int(Fdss.chapter.v(w)), int(Fdss.verse.v(w)))][scroll_name].append(glyphs.replace(' ', ''))\n",
    "            \n",
    "            ids_dss[bhsa_book_name, int(Fdss.chapter.v(w)), int(Fdss.verse.v(w))][scroll_name].append(w)\n",
    "            \n",
    "            # retrieve POS and lexeme of every character of every word in the scrolls and save in a dictionary\n",
    "            for character in glyphs:   \n",
    "                pos_dss[bhsa_book_name, int(Fdss.chapter.v(w)), int(Fdss.verse.v(w))][scroll_name].append(Fdss.sp.v(w))\n",
    "                lexemes_dss[bhsa_book_name, int(Fdss.chapter.v(w)), int(Fdss.verse.v(w))][scroll_name].append(Fdss.glexe.v(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxcQSyeTJSgY"
   },
   "source": [
    "## Prepare BHSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvXZMKLUJSga"
   },
   "source": [
    "The same is done for all the characters in the text of Isaiah in the BHSA. Also, a list with the verses of Isaiah in the BHSA is made. Finally, for each verse, the consonantal structure of each word is collected per verse in the dictionary bhsa_data_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etYcHsMQJSga"
   },
   "outputs": [],
   "source": [
    "all_verses = []\n",
    "lexemes_bhsa = collections.defaultdict(list)\n",
    "pos_bhsa = collections.defaultdict(list)\n",
    "\n",
    "bhsa_data_dict = collections.defaultdict(list)\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    \n",
    "    # Remove cases without consonantal representation. \n",
    "    if F.g_cons.v(w) == '':\n",
    "        continue\n",
    "    \n",
    "    bo, ch, ve = T.sectionFromNode(w)\n",
    "    cl = L.u(w, \"clause\")[0]\n",
    "    words_in_cl = L.d(cl, \"word\")\n",
    "        \n",
    "    # use feature g_cons for consonantal representation of words\n",
    "    bhsa_data_dict[(bo, ch, ve)].append(F.g_cons.v(w))\n",
    "        \n",
    "    # loop over consonants and get lexeme of each consonant in a word\n",
    "    for cons in F.g_cons.v(w):\n",
    "        lexemes_bhsa[(bo, ch, ve)].append(F.lex.v(w)) \n",
    "        pos_bhsa[(bo, ch, ve)].append(F.sp.v(w))\n",
    "        \n",
    "    if (bo, ch, ve) not in all_verses:\n",
    "        all_verses.append((bo, ch, ve))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align verses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the verses are aligned and characters are compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JIIQfFPFJSgd",
    "outputId": "c5061c02-6b21-48bf-e1eb-b8e2f1c0f35e"
   },
   "outputs": [],
   "source": [
    "chars_to_lexemes_etcbc = collections.defaultdict(list)\n",
    "chars_to_lexemes_dss = collections.defaultdict(list)\n",
    "\n",
    "chars_to_pos_etcbc = collections.defaultdict(list)\n",
    "chars_to_pos_dss = collections.defaultdict(list)\n",
    "chars_per_word =  collections.defaultdict(list)\n",
    "\n",
    "all_verses = list(set(all_verses))\n",
    "\n",
    "# loop over verses in BHSA\n",
    "for verse in all_verses:\n",
    "    \n",
    "    # check if verse occurs in the dss package\n",
    "    if verse in dss_data_dict:\n",
    "        \n",
    "        scrolls = (dss_data_dict[verse]).keys()\n",
    "        \n",
    "        for scroll in scrolls:\n",
    "            \n",
    "            #\n",
    "            bhsa_data = bhsa_data_dict[verse]\n",
    "            dss_data = dss_data_dict[verse][scroll]\n",
    "            \n",
    "            # all pairs of verses in the BHSA and scrolls are aligned\n",
    "            bhsa_al, dss_al = align_verses(bhsa_data, dss_data)\n",
    "             \n",
    "            print(verse)\n",
    "            print('BHSA')\n",
    "            print(bhsa_al)\n",
    "            print(scroll)\n",
    "            print(dss_al)\n",
    "            print(' ')\n",
    "        \n",
    "            # some indexes are initialized, these keep track of how many consonants have been observed \n",
    "            # in both aligned sequences\n",
    "            ind_dss = 0\n",
    "            ind_bhsa = 0\n",
    "        \n",
    "            dss_word_ind = 0\n",
    "        \n",
    "            # loop over all characters in the BHSA verse\n",
    "            for pos in range(len(bhsa_al)):\n",
    "            \n",
    "                # for each character in the BHSA sequence, it is cheched what the character is in the DSS sequence.\n",
    "                # Now, a number of scenarios are defined. Most scenarios are not so exciting, e.g.\n",
    "                # if the character is a space in both sequences, then move further\n",
    "                if bhsa_al[pos] == ' ' and dss_al[pos] == ' ':\n",
    "                    \n",
    "                    dss_word_ind += 1\n",
    "            \n",
    "                elif bhsa_al[pos] == '-' and dss_al[pos] == ' ':\n",
    "                    \n",
    "                    dss_word_ind += 1\n",
    "                \n",
    "                elif bhsa_al[pos] == ' ' and dss_al[pos] == '-':\n",
    "                    \n",
    "                    chars_per_word[(scroll, verse, dss_word_ind)].append(dss_al[pos])\n",
    "                               \n",
    "                else:\n",
    "                    if bhsa_al[pos] == '-':\n",
    "                        \n",
    "                        chars_per_word[(scroll, verse, dss_word_ind)].append(dss_al[pos])\n",
    "                        chars_to_lexemes_dss[(scroll, verse, dss_word_ind)].append(lexemes_dss[verse][scroll][ind_dss])\n",
    "                        chars_to_pos_dss[(scroll, verse, dss_word_ind)].append(pos_dss[verse][scroll][ind_dss])\n",
    "                        \n",
    "                        ind_dss += 1\n",
    "                    \n",
    "                    elif dss_al[pos] == '-':\n",
    "                        \n",
    "                        chars_per_word[(scroll, verse, dss_word_ind)].append(dss_al[pos])\n",
    "                        \n",
    "                        ind_bhsa += 1\n",
    "                    \n",
    "                    # Now the real matching is done\n",
    "                    # For a matching consonant, it is checked in the dicts to which lexeme it corresponds\n",
    "                    else:                        \n",
    "                        \n",
    "                        chars_to_lexemes_etcbc[(scroll, verse, dss_word_ind)].append(lexemes_bhsa[verse][ind_bhsa])\n",
    "                        chars_to_pos_etcbc[(scroll, verse, dss_word_ind)].append(pos_bhsa[verse][ind_bhsa])\n",
    "                        \n",
    "                        chars_to_lexemes_dss[(scroll, verse, dss_word_ind)].append(lexemes_dss[verse][scroll][ind_dss])\n",
    "                        chars_to_pos_dss[(scroll, verse, dss_word_ind)].append(pos_dss[verse][scroll][ind_dss])\n",
    "                        chars_per_word[(scroll, verse, dss_word_ind)].append(dss_al[pos])\n",
    "                    \n",
    "                        ind_dss += 1\n",
    "                        ind_bhsa += 1               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexemes and parts of speech are collected and stored in lists, these are converted later to a pandas dataframe.\n",
    "\n",
    "The distionary mapping_dict shows which lexemes in the dss package correspond with their new BHSA alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Qy46BCEJSgg"
   },
   "outputs": [],
   "source": [
    "tf_word_id = [] # tf index\n",
    "scrolls = [] # scroll code\n",
    "books = [] # biblical book name\n",
    "chapters = [] # chapter number\n",
    "verses = [] # verse number\n",
    "words_dss = [] # consonantal representation of word on scroll\n",
    "lexs_dss = [] # lexeme in dss package\n",
    "lexs_etcbc = [] # corresponding etcbc lexeme\n",
    "poss_dss = [] # POS in dss package\n",
    "poss_etcbc = [] # corresponding etcbc POS\n",
    "\n",
    "mapping_dict = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "\n",
    "# loop over all words in the dss, and in each word over all consonants\n",
    "for key in chars_per_word.keys():\n",
    "    \n",
    "    tf_word_id.append(ids_dss[key[1]][key[0]][key[2]])\n",
    "\n",
    "    word = (''.join(chars_per_word[key])).replace(\"-\", \"\")\n",
    "    \n",
    "    lexeme_dss, lexeme_etcbc = produce_value(key, chars_to_lexemes_etcbc, chars_to_lexemes_dss)\n",
    "    pos_dss, pos_etcbc = produce_value(key, chars_to_pos_etcbc, chars_to_pos_dss)\n",
    "    \n",
    "    if lexeme_dss != '':\n",
    "        mapping_dict[lexeme_dss][(key[0], key[1])].append(lexeme_etcbc)\n",
    "    \n",
    "    # collect info in lists\n",
    "    scrolls.append(key[0])\n",
    "    books.append(key[1][0])\n",
    "    chapters.append(key[1][1])\n",
    "    verses.append(key[1][2])\n",
    "    words_dss.append(word)\n",
    "    lexs_dss.append(lexeme_dss)\n",
    "    lexs_etcbc.append(lexeme_etcbc)\n",
    "    poss_dss.append(pos_dss)\n",
    "    poss_etcbc.append(pos_etcbc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following two cells, some empty cells are filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "qCd-W9pwFqhq",
    "outputId": "3ac0810a-1c63-4732-bd3f-26e6b95e6f37"
   },
   "outputs": [],
   "source": [
    "for index, lex in enumerate(lexs_etcbc):\n",
    "  if lex == '':\n",
    "    if lexs_dss[index] != '':\n",
    "        \n",
    "        all_candidates_lists = list((mapping_dict[lexs_dss[index]]).values())\n",
    "        candidates_list = [item for sublist in all_candidates_lists for item in sublist]\n",
    "        \n",
    "        best_cand, count = most_frequent(candidates_list)\n",
    "\n",
    "        lexs_etcbc[index] = best_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_lex_pos = collections.defaultdict(list)\n",
    "\n",
    "for index, lex in enumerate(lexs_etcbc):\n",
    "    \n",
    "    if lex == \"\" or poss_etcbc[index] == \"\":\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        mapping_lex_pos[lex].append(poss_etcbc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check: some lexemes have more than one pos, that needs to be corrected somehow\n",
    "\n",
    "for key in mapping_lex_pos.keys():\n",
    "    if len(set(mapping_lex_pos[key])) > 1:\n",
    "        print(key, collections.Counter(mapping_lex_pos[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(tf_word_id, scrolls, books, chapters, verses, words_dss, lexs_dss, lexs_etcbc, poss_dss, poss_etcbc)), \n",
    "               columns =['tf_word_id', 'scroll','book','chapter', 'verse', 'g_cons', 'lex_dss', 'lex_etcbc', 'pos_dss', 'pos_etcbc']) \n",
    "\n",
    "df_new = df.sort_values(['book', 'scroll', 'chapter', 'verse'], ascending=[True, True, True, True])\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data in a csv file and print the mapping_dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"lexemes_pos_all_bib_books.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L4FAsPDyJSgo",
    "outputId": "c790d166-948e-45fb-8968-3390c6896557"
   },
   "outputs": [],
   "source": [
    "pprint(mapping_dict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mapping_lexemes_all_books.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
