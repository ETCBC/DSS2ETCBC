{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict POS of non-biblical scrolls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a follow up of our blog on etcbc.nl in May 2019. In this notebook a model is trained on the BHSA texts and some scrolls in the extrabiblical package. Predictions of POS are made on the texts of the dss package. Various refinements can be made, but it works so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from statistics import mode\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and load the extrabiblical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(locations='~/github/extrabiblical/tf/0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = TF.load('''\n",
    "    otype mother lex typ code sp book chapter verse label language\n",
    "''')\n",
    "\n",
    "api.loadLog()\n",
    "api.makeAvailableIn(globals())\n",
    "\n",
    "# Give classes a new name. This prevents that they will be overwritten\n",
    "Tx = T\n",
    "Lx = L\n",
    "Fx = F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dss package and give classes a new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.app import use\n",
    "A = use('dss', hoist=globals())\n",
    "\n",
    "Tdss = T\n",
    "Ldss = L\n",
    "Fdss = F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the BHSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.app import use\n",
    "A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function prepare_train_data() the train set is created, and some other useful information is collected. The argument of the function, test_book, is the book which will be excluded from the train set, because it is upon this book that the model will be tested. In the blog we trained on sequences of eight words, which worked well, but it is less convenient in the case of making predictions on the scrolls, because in th scrolls many small pieces of text occur with only a few words. Therefore the training set consists of sequences of 2, 4, 6 and 8 words.\n",
    "\n",
    "Also, 1QHa and 1QS are included in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(test_book):\n",
    "\n",
    "    input_seqs = []\n",
    "    output_pos = []\n",
    "    input_chars = set()\n",
    "    output_vocab = set()\n",
    "\n",
    "    # iterate over all the books\n",
    "    for bo in F.otype.s(\"book\"): \n",
    "        \n",
    "        # exclude the test_book\n",
    "        if F.book.v(bo) == test_book:\n",
    "            continue\n",
    "               \n",
    "        # all the words from a book are collected\n",
    "        words = L.d(bo, 'word')\n",
    "        \n",
    "        # Now we iterate over all the words, except the last words, because all the sequences have to be 8 words long\n",
    "        for w in words[0:-7]:\n",
    "            \n",
    "            languages = [F.g_cons.v(w) for w in range(w, w+8) if (F.g_cons.v(w) != '')]\n",
    "            if 'Aramaic' in languages:\n",
    "                continue\n",
    "            \n",
    "            seqs  = []\n",
    "            \n",
    "            # create sequences of various lengths\n",
    "            for length in [2, 4, 6, 8]:\n",
    "                g_cons_train = (\" \".join([F.g_cons.v(w) for w in range(w, w+length) if (F.g_cons.v(w) != '')])).strip()\n",
    "                parts_of_speech = [F.sp.v(w) for w in range(w, w+length) if (F.g_cons.v(w) != '')]\n",
    "                parts_of_speech = ['\\t'] + parts_of_speech + ['\\n']\n",
    "              \n",
    "                input_seqs.append(g_cons_train)\n",
    "            \n",
    "                output_pos.append(parts_of_speech)\n",
    "\n",
    "                for ch in g_cons_train:\n",
    "                    input_chars.add(ch)\n",
    "            \n",
    "            # also collected is the output vocabulary, which consists of all the parts of speech in the etcbc database\n",
    "                for pos in parts_of_speech:\n",
    "                    output_vocab.add(pos)\n",
    "     \n",
    "    # iterate over all the books of the extrabiblical data\n",
    "    for bo in Fx.otype.s(\"book\"): \n",
    "\n",
    "        if Fx.book.v(bo) not in {'B_1QS', 'B_1QHa'}:\n",
    "            continue\n",
    "               \n",
    "        words = Lx.d(bo, 'word')\n",
    "        \n",
    "        for w in words[0:-7]:\n",
    "            \n",
    "            g_cons_list = []\n",
    "            parts_of_speech = []\n",
    "            \n",
    "            # Here only sequences of 8 words are selected, maybe improve\n",
    "            for w in range(w, w+8): \n",
    "                \n",
    "                languages = [Fx.language.v(w) for w in range(w, w+8) if (Fx.g_cons.v(w) != '')]\n",
    "                if 'Aramaic' in languages:\n",
    "                    continue\n",
    "                \n",
    "                if Fx.g_cons.v(w) in {'', None}:\n",
    "                    continue\n",
    "                           \n",
    "                elif Fx.g_suffix.v(w) == '' and Fx.sp.v(w+1) == 'prps':\n",
    "                    if type(Fx.g_cons.v(w)) == 'str' and type(Fx.g_cons.v(w+ 1)) == 'str':\n",
    "                        g_cons_list.append(Fx.g_cons.v(w) + Fx.g_cons.v(w+1))\n",
    "                        parts_of_speech.append(Fx.sp.v(w))\n",
    "        \n",
    "                elif Fx.g_suffix.v(w-1) == '' and Fx.sp.v(w) == 'prps':\n",
    "                    continue\n",
    "        \n",
    "                else:\n",
    "                    g_cons_list.append(Fx.g_cons.v(w))\n",
    "                    parts_of_speech.append(Fx.sp.v(w))\n",
    "            \n",
    "            if len(g_cons_list) < 6:\n",
    "                continue\n",
    "            \n",
    "            g_cons_train = ' '.join(g_cons_list)   \n",
    "            parts_of_speech = ['\\t'] + parts_of_speech + ['\\n']\n",
    "\n",
    "            for ch in g_cons_train:\n",
    "                input_chars.add(ch)\n",
    "            \n",
    "            # also collected is the output vocabulary, which consists of all the parts of speech in the etcbc database\n",
    "            for pos in parts_of_speech:\n",
    "                output_vocab.add(pos)    \n",
    "                \n",
    "            input_seqs.append(g_cons_train)\n",
    "            output_pos.append(parts_of_speech)             \n",
    "    \n",
    "    input_chars = sorted(list(input_chars))\n",
    "    output_vocab = sorted(list(output_vocab))\n",
    "    \n",
    "    # in the LSTM network all the sequences have to have the same length\n",
    "    max_len_input = max([len(clause) for clause in input_seqs])\n",
    "    max_len_output = max([len(poss) for poss in output_pos])\n",
    "    \n",
    "    # shuffle the data.\n",
    "    input_seqs, output_pos = shuffle(input_seqs, output_pos)\n",
    "    \n",
    "    return input_seqs, output_pos, input_chars, output_vocab, max_len_input, max_len_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(input_voc, output_voc):\n",
    "    \n",
    "    # these dicts map the input sequences\n",
    "    input_idx2char = {}\n",
    "    input_char2idx = {}\n",
    "\n",
    "    for k, v in enumerate(input_voc):\n",
    "        input_idx2char[k] = v\n",
    "        input_char2idx[v] = k\n",
    "     \n",
    "    # and these dicts map the output sequences of parts of speech\n",
    "    output_idx2char = {}\n",
    "    output_char2idx = {}\n",
    "    \n",
    "    for k, v in enumerate(output_voc):\n",
    "        output_idx2char[k] = v\n",
    "        output_char2idx[v] = k\n",
    "        \n",
    "    return input_idx2char, input_char2idx, output_idx2char, output_char2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the final data preparation function is made. Categorical data are generally fed to the LSTM network in one-hot encoded form. The inputs and the outputs have the same length. Also created is an array called decoder_target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(nb_samples, max_len_input, max_len_output, input_chars, output_vocab, input_char2idx, output_char2idx, input_clauses, output_pos):\n",
    "    \n",
    "    # three-dimensional numpy arrays are created \n",
    "    tokenized_input = np.zeros(shape = (nb_samples, max_len_input, len(input_chars)), dtype='float32')\n",
    "    tokenized_output = np.zeros(shape = (nb_samples, max_len_output, len(output_vocab)), dtype='float32')\n",
    "    target_data = np.zeros((nb_samples, max_len_output, len(output_vocab)), dtype='float32')\n",
    "\n",
    "    for i in range(nb_samples):\n",
    "        for k, ch in enumerate(input_clauses[i]):\n",
    "            tokenized_input[i, k, input_char2idx[ch]] = 1\n",
    "        \n",
    "        for k, ch in enumerate(output_pos[i]):\n",
    "            tokenized_output[i, k, output_char2idx[ch]] = 1\n",
    "\n",
    "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "            if k > 0:\n",
    "                target_data[i, k-1, output_char2idx[ch]] = 1\n",
    "                \n",
    "    return tokenized_input, tokenized_output, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function define_LSTM_model() the architecture of the model is created. Neural networks are very flexible structures and a variety of architectures have been developed for various tasks. Here we use the encoder-decoder architecture with two LSTM layers in the encoder. In the architecture there is a variety of hyperparameters that you have to choose. Better hyperparameters lead to better predictions, so it is important to spend time on optimizing this. Hyperparameters in this architecture are the number of LSTM layers, the number of cells in each LSTM layer and the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LSTM_model(input_chars, output_vocab):\n",
    "\n",
    "    # encoder model\n",
    "    encoder_input = Input(shape=(None,len(input_chars)))\n",
    "    encoder_LSTM = LSTM(350,activation='relu',return_state=True, return_sequences=True)(encoder_input)\n",
    "    encoder_LSTM = LSTM(350,return_state=True)(encoder_LSTM)\n",
    "    encoder_outputs, encoder_h, encoder_c = encoder_LSTM\n",
    "    encoder_states = [encoder_h, encoder_c]\n",
    "    \n",
    "    # decoder model\n",
    "    decoder_input = Input(shape=(None,len(output_vocab)))\n",
    "    decoder_LSTM = LSTM(350, return_sequences=True, return_state = True)\n",
    "    decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(len(output_vocab), activation='softmax')\n",
    "    decoder_out = decoder_dense (decoder_out)\n",
    "    \n",
    "    model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return encoder_input, encoder_states, decoder_input, decoder_LSTM, decoder_dense, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is compiled and trained using the function compile_and_train(). The data are fed to the model in small batches. The train data are split in a train and validation set. The latter data consist of 5% of the original train set. The model is trained on the train set, and makes a prediction on these data. The difference between the predictions and the true values of the output are calculated with categorical crossentropy and is called the loss. During training this loss becomes smaller, which means that the predictions become more accurate. However, we want the model not only to become good on the train data, but it should be general enough to make accurate predictions on unseen data. Therefore, after every epoch a prediction is made on the small validation set and the validation loss is calculated. Ideally, the validation loss is more or less equal to the train loss. After a number of epochs, you will notice that the train loss keeps decreasing, while the validation loss remains equal or even increases. At this point the model starts to overfit, which means that the algorithm is modeling idiosyncrasies in the train data instead of general patterns. In that case it is time to stop training and make predictions on the test set.\n",
    "\n",
    "Again, you have to choose a number of hyperparameters. These are the optimizer, the loss function, the batch size, the number of epochs and the learning rate. If you want, you can even tune more hyperparameters.\n",
    "\n",
    "With Earlystopping() the training process can be stopped earlier than the given number of epochs. This is useful if the model starts overfitting and the validation loss does not decrease anymore.\n",
    "\n",
    "Note that training an LSTM model is a computationally intensive process. It is recommended to run the script on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train(model, one_hot_in, one_hot_out, targets, batch_size, epochs, val_split):\n",
    "\n",
    "    callback = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "    adam = Adam(lr=0.00055, beta_1=0.99, beta_2=0.999, epsilon=0.00000001)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "    model.fit(x=[one_hot_in,one_hot_out], \n",
    "              y=targets,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=val_split,\n",
    "              callbacks=[callback])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data are prepared. The test data consist of sequences of words from the book of Nehemiah, so in the preparation of the train data, Nehemiah is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book = \"no_test_book\"\n",
    "\n",
    "input_clauses, output_pos, input_chars, output_vocab, max_len_input, max_len_output = prepare_train_data(test_book)\n",
    "input_idx2char, input_char2idx, output_idx2char, output_char2idx = create_dicts(input_chars, output_vocab)\n",
    "\n",
    "nb_samples = len(input_clauses)\n",
    "one_hot_input, one_hot_output, target_data = one_hot_encode(nb_samples, max_len_input, max_len_output, input_chars, output_vocab, input_char2idx, output_char2idx, input_clauses, output_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_clauses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the functions define_LSTM_model() and compile_and_train() are called. A neural network learns in an iterative process. One iteration is called an epoch. In each iteration a prediction is made, and the train and validation loss are calculated, as you can see in the output.\n",
    "\n",
    "The architecture of the model is also printed with the number of parameters. You also see the number of train samples (397552 samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_input, encoder_states, decoder_input, decoder_LSTM, decoder_dense, model = define_LSTM_model(input_chars, output_vocab)\n",
    "model = compile_and_train(model, one_hot_input, one_hot_output, target_data, 1024, 150, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(350,))\n",
    "decoder_state_input_c = Input(shape=(350,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on scroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which scroll?\n",
    "\n",
    "dss_book = '11Q19'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many srolls are in a fragmentary state, which means that of the text only small pieces remain. The function find_seqs finds places where the text of a scroll is continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seqs(words):\n",
    "    \n",
    "    all_seqs = []\n",
    "    seq = []\n",
    "\n",
    "    for word in words:\n",
    "        \n",
    "        text_word = Tdss.text(word, fmt='text-trans-extra')\n",
    "\n",
    "        if text_word != ' 0  ':\n",
    "            seq.append(word)\n",
    "        else:\n",
    "            if len(seq) > 0:\n",
    "                all_seqs.append(seq)\n",
    "                seq = []\n",
    "                \n",
    "    if len(seq) > 0:\n",
    "        all_seqs.append(seq)\n",
    "                \n",
    "    return(all_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scr in Fdss.otype.s('scroll'):\n",
    "    scroll_name = Tdss.scrollName(scr)\n",
    "\n",
    "    if scroll_name != dss_book:\n",
    "        continue\n",
    "        \n",
    "    words = Ldss.d(scr, 'word')\n",
    "\n",
    "    \n",
    "    all_seqs = find_seqs(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis for the analysis are sequences of 8 words. However, some scattered texts contain shorter sequences. These are processed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_glyphs(all_glyphs, all_lexemes):\n",
    "    \"\"\"\n",
    "    the consonant '#' is used for both 'C' and 'F'. We check in the lexeme\n",
    "    to which of the two alternatives it should be converted. This appproach is crude, \n",
    "    but works well in general.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(all_glyphs)):\n",
    "        \n",
    "        # convert final dss conconants to their etcbc counterparts\n",
    "        all_glyphs[i] = (all_glyphs[i]).replace(u'\\xa0', u' ').replace(\"'\", \"\").replace(\"k\", \"K\").replace(\"n\", \"N\").replace(\"m\", \"M\").replace(\"y\", \"Y\").replace(\"p\", \"P\")\n",
    "        \n",
    "        if '#' not in all_glyphs[i]:\n",
    "            continue\n",
    "                \n",
    "        if all_lexemes[i] == None:\n",
    "            all_glyphs[i] = (all_glyphs[i]).replace('#', 'C') \n",
    "                    \n",
    "        elif 'F' in all_lexemes[i]:\n",
    "            all_glyphs[i] = (all_glyphs[i]).replace('#', 'F')                        \n",
    "\n",
    "        else:\n",
    "            all_glyphs[i] = (all_glyphs[i]).replace('#', 'C')\n",
    "    \n",
    "    return(all_glyphs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data_dss_module(seq, max_len_input):\n",
    "    \"\"\"\n",
    "    Function used for preparation of data from dss package\n",
    "    \"\"\"\n",
    "    \n",
    "    word_nodes = []\n",
    "    input_seqs_test = []\n",
    "    output_seqs_test = []\n",
    "    g_cons_test = []\n",
    "    pos_test = [] \n",
    "    relevant_words = []\n",
    "\n",
    "    if len(seq) < 8:\n",
    "\n",
    "            \n",
    "        #languages = [Fdss.lang.v(seq[w]) for w in seq if (Fdss.glyphe.v(seq[w]) != '')]\n",
    "        #if 'a' in languages or 'g' in languages:\n",
    "        #    return('', [], [])\n",
    "            \n",
    "        all_lexemes = [Fdss.glexe.v(w) for w in seq if (Fdss.glyphe.v(w) not in ('', None))]\n",
    "\n",
    "        all_glyphs = [Fdss.glyphe.v(w) for w in seq if (Fdss.glyphe.v(w) not in ('', None))]\n",
    "            \n",
    "        all_glyphs = preprocess_glyphs(all_glyphs, all_lexemes)\n",
    "\n",
    "            \n",
    "        # excluded are the g_conse values '' and None\n",
    "        g_cons_train = (\" \".join(all_glyphs)).strip()\n",
    "        \n",
    "        parts_of_speech = [Fdss.sp.v(w) for w in seq if (Fdss.glyphe.v(w) not in {'', None})]\n",
    "\n",
    "        parts_of_speech = ['\\t'] + parts_of_speech + ['\\n']\n",
    "\n",
    "        input_seqs_test.append(g_cons_train)\n",
    "\n",
    "        output_seqs_test.append(parts_of_speech)\n",
    "        \n",
    "        nodes = [w for w in seq if (Fdss.glyphe.v(w) not in ('', None))]\n",
    "        word_nodes.append(nodes)\n",
    "    \n",
    "    # now longer sequences are processed\n",
    "    else:\n",
    "\n",
    "        for w in range(len(seq) - 7): \n",
    "            \n",
    "            if Fdss.glyphe.v(seq[w]) in ('', None):\n",
    "                continue\n",
    "            \n",
    "            languages = [Fdss.lang.v(seq[w]) for w in range(w, w+8) if (Fdss.glyphe.v(seq[w]) != '')]\n",
    "            if 'a' in languages:\n",
    "                continue\n",
    "            if 'g' in languages:\n",
    "                continue\n",
    "            \n",
    "            all_lexemes = [Fdss.glexe.v(seq[w]) for w in range(w, w+8) if (Fdss.glyphe.v(seq[w]) not in ('', None) and Tdss.text(seq[w], fmt='text-trans-extra') != \"00 \")]\n",
    "\n",
    "            all_glyphs = [Fdss.glyphe.v(seq[w]) for w in range(w, w+8) if (Fdss.glyphe.v(seq[w]) not in ('', None) and Tdss.text(seq[w], fmt='text-trans-extra') != \"00 \")]\n",
    "\n",
    "            all_glyphs = preprocess_glyphs(all_glyphs, all_lexemes)\n",
    "            \n",
    "            # excluded are the g_conse values '' and None\n",
    "            g_cons_train = (\" \".join(all_glyphs)).strip()\n",
    "\n",
    "            # sometimes greek letters occur, check for this\n",
    "            hebrew = True\n",
    "            for cons in g_cons_train:\n",
    "                if cons not in input_char2idx:\n",
    "                    hebrew = False\n",
    "      \n",
    "            if hebrew == False:\n",
    "                continue\n",
    "            \n",
    "            if len(g_cons_train) > max_len_input:               \n",
    "                continue\n",
    "                \n",
    "            parts_of_speech = [Fdss.sp.v(seq[w]) for w in range(w, w+8) if (Fdss.glyphe.v(seq[w]) not in {'', None} and Tdss.text(seq[w], fmt='text-trans-extra') != \"00 \")]\n",
    "            parts_of_speech = ['\\t'] + parts_of_speech + ['\\n']\n",
    "            \n",
    "            input_seqs_test.append(g_cons_train)\n",
    "\n",
    "            output_seqs_test.append(parts_of_speech)\n",
    "            \n",
    "            nodes = [seq[w] for w in range(w, w+8) if (Fdss.glyphe.v(seq[w]) not in {'', None} and Tdss.text(seq[w], fmt='text-trans-extra') != \"00 \")]           \n",
    "            word_nodes.append(nodes)\n",
    "            \n",
    "    return input_seqs_test, output_seqs_test , word_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode2(nb_samples, max_len_input, max_len_output, input_chars, output_vocab, input_char2idx, output_char2idx, input_clauses, output_pos):\n",
    "    \"\"\"\n",
    "    Function is used for new data, without validation\n",
    "    \"\"\"\n",
    "    \n",
    "    # three-dimensional numpy arrays are created \n",
    "    tokenized_input = np.zeros(shape = (nb_samples, max_len_input, len(input_chars)), dtype='float32')\n",
    "\n",
    "    for i in range(nb_samples):\n",
    "        for k, ch in enumerate(input_clauses[i]):\n",
    "\n",
    "            tokenized_input[i, k, input_char2idx[ch]] = 1\n",
    "\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \"\"\"\n",
    "    This function predicts the POS on the basis of an input sequence\n",
    "    The input is a one-hot encoded sequence of Hebrew words\n",
    "    The output is a list of POS\n",
    "    \"\"\"\n",
    "    \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(output_vocab)))\n",
    "    target_seq[0, 0, output_char2idx['\\t']] = 1\n",
    "    \n",
    "    pred_pos = []\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_out_char = output_idx2char[max_val_index]\n",
    "        pred_pos.append(sampled_out_char)\n",
    "        \n",
    "        if (sampled_out_char == '\\n'):\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(output_vocab)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = collections.defaultdict(list)\n",
    "\n",
    "# loop over continuous pieces of text\n",
    "for seq in all_seqs:\n",
    "\n",
    "    if len(seq) == 0:\n",
    "        continue\n",
    "    \n",
    "    # exclude 'interpunction signs'\n",
    "    seq = [w for w in seq if Tdss.text(w, fmt='text-trans-extra') != \"00 \"]\n",
    "    \n",
    "    # prepare data and create one-hot encoding\n",
    "    input_seqs_dss, output_seqs_test_dss, words_list = prepare_test_data_dss_module(seq, max_len_input)\n",
    "    one_hot_dss = one_hot_encode2(len(input_seqs_dss), max_len_input, max_len_output, input_chars, output_vocab, input_char2idx, output_char2idx, input_seqs_dss, output_seqs_test_dss)\n",
    "\n",
    "    print(dss_book, len(input_seqs_dss), one_hot_dss.shape)\n",
    "\n",
    "    for seq_index in range(len(one_hot_dss)):\n",
    "    \n",
    "        if len(words_list[seq_index]) == 0:\n",
    "            continue\n",
    "            \n",
    "        inp_seq = one_hot_dss[seq_index:seq_index+1]\n",
    "    \n",
    "        pred_pos = decode_seq(inp_seq)\n",
    "        pred_pos = pred_pos[:-1]\n",
    "\n",
    "        print(input_seqs_dss[seq_index])\n",
    "        print(pred_pos)\n",
    "        print(' ')\n",
    "        \n",
    "        if len(words_list[seq_index]) != len(pred_pos):\n",
    "            continue\n",
    "            \n",
    "        for pred_ind in range(len(pred_pos)):\n",
    "            all_predictions[words_list[seq_index][pred_ind]].append(pred_pos[pred_ind]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(list(all_predictions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(item_list): \n",
    "    occurence_count = Counter(item_list) \n",
    "    return occurence_count.most_common(1)[0][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_word_id = []\n",
    "glyphe = []\n",
    "poss_dss = []\n",
    "pos_etcbc = []\n",
    "\n",
    "for key in all_words:\n",
    "\n",
    "    if len(all_predictions[key]) == 0:\n",
    "        continue\n",
    "        \n",
    "    data = collections.Counter(all_predictions[key])\n",
    "\n",
    "    pos_dss = Fdss.sp.v(key)\n",
    "    print(key , Fdss.glyphe.v(key), Fdss.sp.v(key), most_frequent(all_predictions[key]))\n",
    "    \n",
    "    tf_word_id.append(key)\n",
    "    glyphe.append(Fdss.glyphe.v(key))\n",
    "    poss_dss.append(pos_dss)\n",
    "    \n",
    "    # if the dss package says the pos is unknown, we adopt that and overrule our prediction\n",
    "    if pos_dss == 'unknown':\n",
    "        pos_etcbc.append('unknown')\n",
    "        \n",
    "    else:\n",
    "        pos_etcbc.append(most_frequent(all_predictions[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss_df = pd.DataFrame(list(zip(tf_word_id, glyphe, poss_dss, pos_etcbc)), \n",
    "               columns =['tf_word_id', 'g_cons', 'pos_dss', 'pos_etcbc']) \n",
    "\n",
    "file_name = dss_book + '_pos.csv'\n",
    "\n",
    "dss_df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scr in Fdss.otype.s('scroll'):\n",
    "    scroll_name = Tdss.scrollName(scr)\n",
    "    print(scroll_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputest",
   "language": "python",
   "name": "gputest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
