{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding clause boundaries in the biblical scrolls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script clause boundaries in the BHSA are transfered to the biblical Dead Sea Scrolls by using sequence alignment. This approach works well, as long as clause boundaries in the BHSA correspond with clause boundaries in the scrolls. By far most clause boundaries are placed correctly, but further processing is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "from Bio.Seq import Seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.app import use\n",
    "A = use('dss', hoist=globals())\n",
    "\n",
    "Ldss = L\n",
    "Tdss = T\n",
    "Fdss = F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.app import use\n",
    "A = use('bhsa', hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper function my_split splits a string on any of a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_split(s, seps):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    s is a string\n",
    "    seps is a list of strings. E.g. is seps = [' ', 'b'], the input string s is split on both a space\n",
    "    and the letter 'b'\n",
    "    Output:\n",
    "    res is a list containg the original string and the resulting substrings.\n",
    "    \"\"\"\n",
    "    res = [s]\n",
    "    for sep in seps:\n",
    "        s, res = res, []\n",
    "        for seq in s:\n",
    "            res += seq.split(sep)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss_isa_dict = collections.defaultdict(list)\n",
    "\n",
    "dss_isa_dict_ch = collections.defaultdict(list)\n",
    "\n",
    "lexemes_dss = collections.defaultdict(list)\n",
    "\n",
    "for scr in Fdss.otype.s('scroll'):\n",
    "    scroll_name = Tdss.scrollName(scr)\n",
    "    \n",
    "    if scroll_name == '1Qisaa':\n",
    "        words = Ldss.d(scr, 'word')\n",
    "        \n",
    "        for w in words:\n",
    "            \n",
    "            if Fdss.glyphe.v(w) != None:\n",
    "                lexeme = Fdss.glexe.v(w)\n",
    "                glyphs = Fdss.glyphe.v(w)\n",
    "                        \n",
    "                # the consonant '#' is used for both 'C' and 'F'. We check in the lexeme\n",
    "                # to which of the two alternatives it should be converted. This appproach is crude, \n",
    "                # but works well in practice. There is only one word with both F and C in the lexeme: >RTX##T> >AR:T.AX:CAF:T.:> in 4Q117\n",
    "                if '#' in glyphs:  \n",
    "                    # hardcode the single word with both 'C' and 'F' in the lexeme.\n",
    "                    if glyphs == '>RTX##T>':\n",
    "                        glyphs = '>RTXCFT>'\n",
    "                    \n",
    "                    elif 'F' in lexeme:\n",
    "                        glyphs = glyphs.replace('#', 'F')\n",
    "                    \n",
    "                    # cases in wich 'C' occurs in lexeme or morphology\n",
    "                    else:                        \n",
    "                        glyphs = glyphs.replace('#', 'C')\n",
    "                        \n",
    "                glyphs = glyphs.replace(u'\\xa0', u' ').replace(\"'\", \"\").replace(\"k\", \"K\").replace(\"n\", \"N\").replace(\"m\", \"M\").replace(\"y\", \"Y\").replace(\"p\", \"P\")   \n",
    "               \n",
    "                dss_isa_dict[(int(Fdss.chapter.v(w)), int(Fdss.verse.v(w)))].append(glyphs)\n",
    "                dss_isa_dict_ch[int(Fdss.chapter.v(w))].append(glyphs)\n",
    "                lexemes_dss[(int(Fdss.chapter.v(w)), int(Fdss.verse.v(w)))].append(Fdss.glexe.v(w)) \n",
    "                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clause endings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BHSA data are prepared, by constructing the text of verses, with an 'e' inserted after a clause has ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verses = []\n",
    "lexemes_bhsa = collections.defaultdict(list)\n",
    "\n",
    "bhsa_isa_dict = collections.defaultdict(list)\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    bo, ch, ve = T.sectionFromNode(w)\n",
    "    if bo == 'Isaiah':\n",
    "        \n",
    "        cl = L.u(w, \"clause\")[0]\n",
    "        words_in_cl = L.d(cl, \"word\")\n",
    "        \n",
    "        bhsa_isa_dict[(ch, ve)].append(F.g_cons.v(w))\n",
    "        for cons in F.g_cons.v(w):\n",
    "            lexemes_bhsa[(ch, ve)].append(F.lex.v(w))        \n",
    "        \n",
    "        # add end (e) of clause\n",
    "        if w == words_in_cl[-1]:\n",
    "            bhsa_isa_dict[(ch, ve)].append('e')\n",
    "        if (ch, ve) not in all_verses:\n",
    "            all_verses.append((ch, ve))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verses_split1 = []\n",
    "\n",
    "words_dss = []\n",
    "cl_ends = []\n",
    "verse_list = []\n",
    "\n",
    "for verse in all_verses:\n",
    "        \n",
    "    if verse in dss_isa_dict:\n",
    "        \n",
    "        seq_dss = ' '.join(dss_isa_dict[verse])\n",
    "        seq_bhsa = ' '.join(bhsa_isa_dict[verse])\n",
    "        \n",
    "        seq1 = Seq(seq_bhsa) \n",
    "        seq2 = Seq(seq_dss)\n",
    "    \n",
    "        alignments = pairwise2.align.globalxx(seq1, seq2)\n",
    "        \n",
    "        dss_al = alignments[0][1]\n",
    "        bhsa_al = alignments[0][0]\n",
    "        \n",
    "        print(verse)\n",
    "        print(dss_al)\n",
    "        print(bhsa_al)\n",
    "        \n",
    "        dss_list = list(dss_al)\n",
    "    \n",
    "        all_endings = [m.start() for m in re.finditer('e', bhsa_al)]\n",
    "\n",
    "        dss_list = list(dss_al)\n",
    "\n",
    "        for ending in all_endings:\n",
    "            dss_list[ending] = 'e'\n",
    "        \n",
    "        dss_new = ''.join(dss_list)\n",
    "        \n",
    "        clauses_dss = my_split(dss_new, [\"-e \", \" e \", \" e\", \"-e-- \", \"-e- \"])   \n",
    "\n",
    "        print(clauses_dss)\n",
    "\n",
    "        clauses_dss2 = []\n",
    "        \n",
    "        for cl in clauses_dss:\n",
    "            cl = cl.strip('e')\n",
    "    \n",
    "            if not 'e' in cl:\n",
    "                cl = cl.replace(\"-\", \"\")\n",
    "    \n",
    "                clauses_dss2.append(cl)\n",
    "        \n",
    "            else:\n",
    "                \n",
    "                # '>M J-D-WMW K- TWL<-eT K- YMR JHJW'\n",
    "                all_starts_b = [m.start() for m in re.finditer('e', cl)]\n",
    "                all_starts_sp = [m.start() for m in re.finditer(' ', cl)]\n",
    "        \n",
    "                if len(all_starts_sp) == 0:\n",
    "                        \n",
    "                    cl = cl.replace('e', '')\n",
    "                    cl = cl.replace('-', '')\n",
    "                    clauses_dss2.append(cl)\n",
    "                        \n",
    "                else: \n",
    "                        \n",
    "                    indices = [0]\n",
    "                    for beg in all_starts_b:\n",
    "                        if max(all_starts_sp) > beg:\n",
    "                            indices.append(min([sp for sp in all_starts_sp if sp > beg]))\n",
    "  \n",
    "                    parts = [cl[i:j] for i,j in zip(indices, indices[1:]+[None])]\n",
    "    \n",
    "                    for part in parts:\n",
    "                        clauses_dss2.append(part)\n",
    "                \n",
    "        cl_dss2 = [cl.replace(\"-\", \"\").strip(\" \").split(\" \") for cl in clauses_dss2] #.replace('e', '')  \n",
    "        print(cl_dss2)\n",
    "        \n",
    "        for c in cl_dss2:\n",
    "            if c == [\"\"] or c == [\"e\"]:\n",
    "                continue\n",
    "            for ind, w in enumerate(c):\n",
    "                words_dss.append(w)\n",
    "                verse_list.append(verse)\n",
    "\n",
    "                if ind == len(c) - 1:\n",
    "                    cl_ends.append('e')\n",
    "                else:\n",
    "                    cl_ends.append('-')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clause starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verses = []\n",
    "lexemes_bhsa = collections.defaultdict(list)\n",
    "\n",
    "bhsa_isa_dict = collections.defaultdict(list)\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    bo, ch, ve = T.sectionFromNode(w)\n",
    "    if bo == 'Isaiah':\n",
    "        \n",
    "        cl = L.u(w, \"clause\")[0]\n",
    "        words_in_cl = L.d(cl, \"word\")\n",
    "        \n",
    "        # add beginning (b) of clause\n",
    "        if w == words_in_cl[0]:\n",
    "        \n",
    "            bhsa_isa_dict[(ch, ve)].append('b')\n",
    "        \n",
    "        bhsa_isa_dict[(ch, ve)].append(F.g_cons.v(w))\n",
    "        for cons in F.g_cons.v(w):\n",
    "            lexemes_bhsa[(ch, ve)].append(F.lex.v(w))\n",
    "        \n",
    "        if (ch, ve) not in all_verses:\n",
    "            all_verses.append((ch, ve))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verses_split1 = []\n",
    "\n",
    "words_dss2 = []\n",
    "cl_starts = []\n",
    "\n",
    "\n",
    "for verse in all_verses:\n",
    "\n",
    "    if verse in dss_isa_dict:\n",
    "        print(verse)\n",
    "        \n",
    "        seq_dss = ' '.join(dss_isa_dict[verse])\n",
    "        seq_bhsa = ' '.join(bhsa_isa_dict[verse])\n",
    "\n",
    "        seq1 = Seq(seq_dss) \n",
    "        seq2 = Seq(seq_bhsa)\n",
    "        \n",
    "        print(seq1)\n",
    "        print(seq2)\n",
    "    \n",
    "        alignments = pairwise2.align.globalxx(seq2, seq1)\n",
    "        \n",
    "        bhsa_al = alignments[0][0]\n",
    "        dss_al = alignments[0][1]\n",
    "\n",
    "        dss_list = list(dss_al)\n",
    "    \n",
    "        all_endings = [m.start() for m in re.finditer('b', bhsa_al)]\n",
    "\n",
    "        dss_list = list(dss_al)\n",
    "\n",
    "        for ending in all_endings:\n",
    "            dss_list[ending] = 'b'\n",
    "        \n",
    "        dss_new = ''.join(dss_list)\n",
    "        \n",
    "        clauses_dss = my_split(dss_new, [\"-b \", \" b\", \"b \", \"-b- \", \"-b-- \"])\n",
    "\n",
    "        clauses_dss2 = []\n",
    "        \n",
    "        for cl in clauses_dss:\n",
    "            cl = cl.strip('b')\n",
    "    \n",
    "            if not 'b' in cl:\n",
    "                cl = cl.replace(\"-\", \"\")\n",
    "    \n",
    "                clauses_dss2.append(cl)\n",
    "        \n",
    "            else:\n",
    "                # '>M J-D-WMW K- TWL<-eT K- YMR JHJW'\n",
    "                all_starts_b = [m.start() for m in re.finditer('b', cl)]\n",
    "                all_starts_sp = [m.start() for m in re.finditer(' ', cl)]\n",
    "                \n",
    "                if len(all_starts_sp) == 0:\n",
    "                        \n",
    "                    cl = cl.replace('b', '')\n",
    "                    cl = cl.replace('-', '')\n",
    "                    clauses_dss2.append(cl)\n",
    "                        \n",
    "                else: \n",
    "                        \n",
    "                    indices = [0]\n",
    "                    for beg in all_starts_b:\n",
    "                        if max(all_starts_sp) > beg:\n",
    "                            indices.append(min([sp for sp in all_starts_sp if sp > beg]))\n",
    "                    print(indices)\n",
    "                    print('clause', cl)  \n",
    "                    parts = [cl[i:j] for i,j in zip(indices, indices[1:]+[None])]\n",
    "                    print('parts', parts)\n",
    "                    for part in parts:\n",
    "                        clauses_dss2.append(part)\n",
    "\n",
    "        cl_dss2 = [cl.strip('b').replace(\"b\", \"\").replace(\"-\", \"\").strip(\" \").split(\" \") for cl in clauses_dss2]\n",
    "        \n",
    "        print('cl_dss2', cl_dss2)\n",
    "        for clause in cl_dss2:\n",
    "            \n",
    "            if clause == [\"\"] or clause == [\"b\"]:\n",
    "                continue\n",
    "            for ind, w1 in enumerate(clause):\n",
    "                \n",
    "                words_dss2.append(w1)\n",
    "\n",
    "                if ind == 0:\n",
    "                    cl_starts.append('b')\n",
    "                else:\n",
    "                    cl_starts.append('-')\n",
    "\n",
    "print(len(words_dss2))\n",
    "print(len(cl_starts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words_dss2)):\n",
    "    print(words_dss2[i], words_dss[i], verse_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_verse = [[ ch for ch, v in verse_list ], \n",
    "       [ v for ch, v in verse_list ]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "isa_df = pd.DataFrame(list(zip(chap_verse[0], chap_verse[1], words_dss2, cl_starts, cl_ends)), \n",
    "               columns =['chapter', 'verse', 'word', 'start_of_clause', 'end_of_clause']) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isa_df.to_csv(\"_1_q_isaa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
